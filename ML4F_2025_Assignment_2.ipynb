{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Case 2 -- Machine Learning for Finance 2025\n",
    "\n",
    "<span style='color:crimson; font-weight: bold'>Submission deadline: Friday, 5 December 2025, 22:00 pm AMS. </span>\n",
    "\n",
    "## Instructions\n",
    "* This case covers the material discussed in wk2-wk5.\n",
    "* Do not forget to create a group again, go to Canvas -> people -> groups -> Case2\n",
    "* Each group submits _only one_ notebook via canvas on the assignment page. Only Jupyter notebooks are accepted via a group. \n",
    "* The notebook should be named `case2_groupXX.ipynb` where `XX` is your group number,  \n",
    "e.g. for group 3 this will be `case2_group03.ipynb`.\n",
    "* Make sure you download the correct dataset. Loading the wrong set will lead to deduction of points. \n",
    "* The notebook should run without raising any errors.\n",
    "* Deadline: **Friday 5 December 22:00 (AMS)**. Not meeting the deadline gives a discount of 10 pt per hour\n",
    "* Standard plagiarism and AI checks are in place\n",
    "* As a standard anti-fraud measure, I can at random select a number of you to explain your code \n",
    "and answers. Any one of you must be able to explain any part of the code. Failure to explain \n",
    "your answers will result in a deduction of credits for this case for the whole group. Each group is responsible for all group members being able to explain any part the code\n",
    "* If you need to make a Table or Figure, do this in JF-style. (hence provide a sufficient caption explaining (NOT interpreting) what is in the Figure/Table)\n",
    "* If you test something, provide H0/HA, the test statistic (formula and number) and your conclusion.\n",
    "* Do not spend time on optimizing the speed of your code. However, if it runs for more than 5 minutes, we will terminate \n",
    "it.\n",
    "\n",
    "----\n",
    "\n",
    "<div style=\"font-size:24px; text-align:center; font-weight:bold\">Good luck!</div>\n",
    "\n",
    "----\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Case 2 - Modeling defaults\n",
    "\n",
    "In this case you will model the defaults of U.K. companies by using several classification models. \n",
    "The goal is to see whether sophisticated machine learning models such as decision trees, neural networks and ensemble learning methods can beat a simple logit model. \n",
    "\n",
    "Download your own dataset from Canvas. You have the following variables at your disposal:\n",
    "* **org_id**: organisation ID\n",
    "* **sic2**: 2-Digit SIC (Standard Industrial Classification) Codes\n",
    "* **year**: time in year\n",
    "* **def**: binary indicator: 1 if the company defaulted, 0 else\n",
    "* **wkta**: working capital over total assets\n",
    "* **reta**: retained earnings over total assets\n",
    "* **ebitta**: earnings before interest and taxes over total assets \n",
    "* **mv**: the market to book value\n",
    "  \n",
    "First take a look at the data, then test different algorithms, make predictions and provide an answer to the main questios of the case\n",
    "\n",
    "State your imports below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "\n",
    "# Scikit-learn libraries\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier, plot_tree\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.metrics import (accuracy_score, recall_score, precision_score, \n",
    "                             f1_score, fbeta_score, confusion_matrix, \n",
    "                             classification_report)\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Neural Network libraries\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Load the dataset from Excel\n",
    "df = pd.read_excel('data_MLF_case2_group_26.xlsx', sheet_name='Data')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part I: Preprocessing (15 points)\n",
    "\n",
    "Import your data from excel. Then perform the following tasks:\n",
    "1. Check for missing values and delete these.\n",
    "2. Do the features contain any outliers? If so, treat them carefully with an explanation.\n",
    "3. Show the correlations between the (labels and the) features. Comment on the sign of 2 randomly selected features\n",
    "4. Show summary statistics in **Table 1**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   org_id  sic2  year  def      wkta      reta    ebitta        mv\n",
      "0    3671    33  2001    0  0.153244  0.113063  0.070824  1.806667\n",
      "1    3174    59  2001    0  0.380485  0.203497  0.126315  1.734991\n",
      "2    3289    78  1988    0  0.039873  0.000000  0.031834  0.196416\n",
      "3    7872    48  2001    0  0.202144 -0.160948  0.062240 -0.238152\n",
      "4    1275    49  1991    0 -0.510000 -0.260833  0.025862 -0.130025\n",
      "(1025, 8)\n",
      "\n",
      "1. Dealing with missing values\n",
      "\n",
      "org_id     0\n",
      "sic2       0\n",
      "year       0\n",
      "def        0\n",
      "wkta      42\n",
      "reta      42\n",
      "ebitta    42\n",
      "mv        42\n",
      "dtype: int64\n",
      "\n",
      "Total missing values: 168\n",
      "\n",
      "Original dataset size: 1025\n",
      "Cleaned dataset size: 983\n",
      "Rows removed: 42\n"
     ]
    }
   ],
   "source": [
    "### 1\n",
    "\n",
    "print(df.head())\n",
    "print(df.shape)\n",
    "\n",
    "print(\"\")\n",
    "print(\"1. Dealing with missing values\")\n",
    "print(\"\")\n",
    "# Check for missing values\n",
    "print(df.isnull().sum())\n",
    "print(f\"\\nTotal missing values: {df.isnull().sum().sum()}\")\n",
    "\n",
    "# Delete rows with missing values\n",
    "df_clean = df.dropna()\n",
    "\n",
    "print(f\"\\nOriginal dataset size: {len(df)}\")\n",
    "print(f\"Cleaned dataset size: {len(df_clean)}\")\n",
    "print(f\"Rows removed: {len(df) - len(df_clean)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. 42 rows were found with missing values and subsequently deleted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (182529269.py, line 7)",
     "output_type": "error",
     "traceback": [
      "  \u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[15]\u001b[39m\u001b[32m, line 7\u001b[39m\n\u001b[31m    \u001b[39m\u001b[31mdf_def = df_clean(def==1)\u001b[39m\n                      ^\n\u001b[31mSyntaxError\u001b[39m\u001b[31m:\u001b[39m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "### 2\n",
    "\n",
    "print(\"\")\n",
    "print(\"2. Looking at outliers\")\n",
    "print(\"\")\n",
    "print(df_clean.describe())\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part II: Training (50 points)\n",
    "\n",
    "We've familiarized ourselves with the data, so now we're going to train some models to model the probability of default. \n",
    "Use *wkta, reta, ebitta* and *mv* as features in the following models:\n",
    "- Model 1: The logistic classifier \n",
    "- Model 2: The decision tree\n",
    "- Model 3: A neural network\n",
    "- Model 4: Gradient Boosting for Classicifation\n",
    "\n",
    "Split the data into a random training, validation and test using using the 60/20/20 rule. Pin down your random sets by providing the seed.\n",
    "If there are hyperparameters, tune these in the correct way and show plots - using an appropriate measure - to explain your final hyperparameter(s). Explain why you have used this measure!\n",
    "\n",
    "Create **Table 2** by showing the accuracy, recall, precision, F1 score, and the $F_\\beta$ using the test set for each model.\n",
    "Choose your own $\\beta$ with explanation. Interpret the outcomes. Did you expect these results? Why/Why NOT?\n",
    "\n",
    "### More information about Model 2\n",
    "Estimate a dicision tree using the default hyperparameteres of Python. Just tune the *maximum depth* parameter. \n",
    "\n",
    "### More information about Model 3\n",
    "Set the following conditions fixed: \n",
    "- **Activation function**: *Relu* for hidden layers\n",
    "- **dropout ratio**: put this after each hidden layer and set it equal to 0.2\n",
    "- **epochs**: 50\n",
    "- **batch size**: 10\n",
    "\n",
    "When compiling the model, set the following conditions fixed:\n",
    "- Use loss = 'categorical_crossentropy'\n",
    "- optimizer='adam'\n",
    "- metrics=accuracy\n",
    "\n",
    "Now tune the following hyperparameters: \n",
    "- the number of **hidden** layers: 1 or 2.\n",
    "- Number of nodes per hidden layer: 16 or 8. \n",
    "\n",
    "### More information about Model 4\n",
    "Set the following hyperparameters fixed:\n",
    "- Learning rate: 0.8\n",
    "- random state: 0\n",
    "- max depth: 2\n",
    "\n",
    "Tune the the parameter *n_estimators*.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part III: Estimating a tree using only two features (30 points)\n",
    "\n",
    "Select only **wkta** and **reta** as features. Then estimate again a decision tree, again tuning the *maximum depth* hyperparameter.\n",
    "Lets call this model **Decision Tree Small**\n",
    "\n",
    "Answer the following questions:\n",
    "- Is the hyperparameter of your decision tree of Part II robust?\n",
    "- Does Decision Tree Small outperform the best model of Part II?\n",
    "- Does Decision Tree Small beat the logit *with the same two features*?\n",
    "\n",
    "Also make a plot of your final decision tree with the two features as done in the tutorial/lecture. Interpret this figure. \n",
    "\n",
    "Put your test results in **Table 3**. Interpret your results and relate these to main question of the case.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Discussion (5 points)\n",
    "\n",
    "Provide a short discussion about the way we tune the hyperparameters. Pay attention to the charactaristics of the data at hand while answering this question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
