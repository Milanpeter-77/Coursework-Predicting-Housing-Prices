{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Case 2 -- Machine Learning for Finance 2025\n",
    "\n",
    "<span style='color:crimson; font-weight: bold'>Submission deadline: Friday, 5 December 2025, 22:00 pm AMS. </span>\n",
    "\n",
    "## Instructions\n",
    "* This case covers the material discussed in wk2-wk5.\n",
    "* Do not forget to create a group again, go to Canvas -> people -> groups -> Case2\n",
    "* Each group submits _only one_ notebook via canvas on the assignment page. Only Jupyter notebooks are accepted via a group. \n",
    "* The notebook should be named `case2_groupXX.ipynb` where `XX` is your group number,  \n",
    "e.g. for group 3 this will be `case2_group03.ipynb`.\n",
    "* Make sure you download the correct dataset. Loading the wrong set will lead to deduction of points. \n",
    "* The notebook should run without raising any errors.\n",
    "* Deadline: **Friday 5 December 22:00 (AMS)**. Not meeting the deadline gives a discount of 10 pt per hour\n",
    "* Standard plagiarism and AI checks are in place\n",
    "* As a standard anti-fraud measure, I can at random select a number of you to explain your code \n",
    "and answers. Any one of you must be able to explain any part of the code. Failure to explain \n",
    "your answers will result in a deduction of credits for this case for the whole group. Each group is responsible for all group members being able to explain any part the code\n",
    "* If you need to make a Table or Figure, do this in JF-style. (hence provide a sufficient caption explaining (NOT interpreting) what is in the Figure/Table)\n",
    "* If you test something, provide H0/HA, the test statistic (formula and number) and your conclusion.\n",
    "* Do not spend time on optimizing the speed of your code. However, if it runs for more than 5 minutes, we will terminate \n",
    "it.\n",
    "\n",
    "----\n",
    "\n",
    "<div style=\"font-size:24px; text-align:center; font-weight:bold\">Good luck!</div>\n",
    "\n",
    "----\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Case 2 - Modeling defaults\n",
    "\n",
    "In this case you will model the defaults of U.K. companies by using several classification models. \n",
    "The goal is to see whether sophisticated machine learning models such as decision trees, neural networks and ensemble learning methods can beat a simple logit model. \n",
    "\n",
    "Download your own dataset from Canvas. You have the following variables at your disposal:\n",
    "* **org_id**: organisation ID\n",
    "* **sic2**: 2-Digit SIC (Standard Industrial Classification) Codes\n",
    "* **year**: time in year\n",
    "* **def**: binary indicator: 1 if the company defaulted, 0 else\n",
    "* **wkta**: working capital over total assets\n",
    "* **reta**: retained earnings over total assets\n",
    "* **ebitta**: earnings before interest and taxes over total assets \n",
    "* **mv**: the market to book value\n",
    "  \n",
    "First take a look at the data, then test different algorithms, make predictions and provide an answer to the main questios of the case\n",
    "\n",
    "State your imports below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 19\u001b[39m\n\u001b[32m     16\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msklearn\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpreprocessing\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m StandardScaler\n\u001b[32m     18\u001b[39m \u001b[38;5;66;03m# Neural Network libraries\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m19\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtensorflow\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m keras\n\u001b[32m     20\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtensorflow\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mkeras\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmodels\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Sequential\n\u001b[32m     21\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtensorflow\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mkeras\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mlayers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Dense, Dropout\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'tensorflow'"
     ]
    }
   ],
   "source": [
    "# Standard libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "\n",
    "# Scikit-learn libraries\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier, plot_tree\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.metrics import (accuracy_score, recall_score, precision_score, \n",
    "                             f1_score, fbeta_score, confusion_matrix, \n",
    "                             classification_report)\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Neural Network libraries\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Load the dataset from Excel\n",
    "df = pd.read_excel('data_MLF_case2_group_26.xlsx', sheet_name='Data')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part I: Preprocessing (15 points)\n",
    "\n",
    "Import your data from excel. Then perform the following tasks:\n",
    "1. Check for missing values and delete these.\n",
    "2. Do the features contain any outliers? If so, treat them carefully with an explanation.\n",
    "3. Show the correlations between the (labels and the) features. Comment on the sign of 2 randomly selected features\n",
    "4. Show summary statistics in **Table 1**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   org_id  sic2  year  def      wkta      reta    ebitta        mv\n",
      "0    3671    33  2001    0  0.153244  0.113063  0.070824  1.806667\n",
      "1    3174    59  2001    0  0.380485  0.203497  0.126315  1.734991\n",
      "2    3289    78  1988    0  0.039873  0.000000  0.031834  0.196416\n",
      "3    7872    48  2001    0  0.202144 -0.160948  0.062240 -0.238152\n",
      "4    1275    49  1991    0 -0.510000 -0.260833  0.025862 -0.130025\n",
      "(1025, 8)\n",
      "\n",
      "1. Dealing with missing values\n",
      "\n",
      "org_id     0\n",
      "sic2       0\n",
      "year       0\n",
      "def        0\n",
      "wkta      42\n",
      "reta      42\n",
      "ebitta    42\n",
      "mv        42\n",
      "dtype: int64\n",
      "\n",
      "Total missing values: 168\n",
      "\n",
      "Original dataset size: 983\n",
      "Cleaned dataset size: 983\n",
      "Rows removed: 0\n"
     ]
    }
   ],
   "source": [
    "### 1\n",
    "\n",
    "print(df.head())\n",
    "print(df.shape)\n",
    "\n",
    "print(\"\")\n",
    "print(\"1. Dealing with missing values\")\n",
    "print(\"\")\n",
    "# Check for missing values\n",
    "print(df.isnull().sum())\n",
    "print(f\"\\nTotal missing values: {df.isnull().sum().sum()}\")\n",
    "\n",
    "# Delete rows with missing values\n",
    "df = df.dropna()\n",
    "\n",
    "print(f\"\\nOriginal dataset size: {len(df)}\")\n",
    "print(f\"Cleaned dataset size: {len(df)}\")\n",
    "print(f\"Rows removed: {len(df) - len(df)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. 42 rows were found with missing values and subsequently deleted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "2. Looking at outliers\n",
      "\n",
      "            org_id        sic2         year         def        wkta  \\\n",
      "count   983.000000  983.000000   983.000000  983.000000  983.000000   \n",
      "mean   3514.200407   42.203459  1995.636826    0.107833    0.134980   \n",
      "std    2653.021723   18.547547     6.745093    0.310327    0.231457   \n",
      "min       4.000000    1.000000  1981.000000    0.000000   -0.510000   \n",
      "25%    1213.000000   29.000000  1989.000000    0.000000    0.009800   \n",
      "50%    2825.000000   38.000000  1999.000000    0.000000    0.112705   \n",
      "75%    5784.000000   50.000000  2001.000000    0.000000    0.273275   \n",
      "max    9147.000000   99.000000  2002.000000    1.000000    0.730000   \n",
      "\n",
      "             reta      ebitta          mv  \n",
      "count  983.000000  983.000000  983.000000  \n",
      "mean     0.091985    0.066591    0.604055  \n",
      "std      0.390075    0.106839    1.558802  \n",
      "min     -1.220000   -0.330000   -3.780000  \n",
      "25%     -0.050325    0.026088   -0.029801  \n",
      "50%      0.115826    0.079463    0.845793  \n",
      "75%      0.270008    0.119439    1.582276  \n",
      "max      1.560000    0.400000    4.160000  \n",
      "\n",
      "Outlier Detection using IQR Method:\n",
      "============================================================\n",
      "\n",
      "wkta:\n",
      "  Lower bound: -0.3854\n",
      "  Upper bound: 0.6685\n",
      "  Number of outliers: 55 (5.60%)\n",
      "\n",
      "reta:\n",
      "  Lower bound: -0.5308\n",
      "  Upper bound: 0.7505\n",
      "  Number of outliers: 105 (10.68%)\n",
      "\n",
      "ebitta:\n",
      "  Lower bound: -0.1139\n",
      "  Upper bound: 0.2595\n",
      "  Number of outliers: 86 (8.75%)\n",
      "\n",
      "mv:\n",
      "  Lower bound: -2.4479\n",
      "  Upper bound: 4.0004\n",
      "  Number of outliers: 64 (6.51%)\n"
     ]
    }
   ],
   "source": [
    "### 2\n",
    "\n",
    "print(\"\")\n",
    "print(\"2. Looking at outliers\")\n",
    "print(\"\")\n",
    "print(df.describe())\n",
    "\n",
    "# Define features for outlier detection\n",
    "features = ['wkta', 'reta', 'ebitta', 'mv']\n",
    "\n",
    "# Calculate outliers using IQR method\n",
    "print(\"\\nOutlier Detection using IQR Method:\")\n",
    "print(\"=\"*60)\n",
    "for feature in features:\n",
    "    Q1 = df[feature].quantile(0.25)\n",
    "    Q3 = df[feature].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    lower_bound = Q1 - 1.5 * IQR\n",
    "    upper_bound = Q3 + 1.5 * IQR\n",
    "    \n",
    "    outliers = df[(df[feature] < lower_bound) | (df[feature] > upper_bound)]\n",
    "    print(f\"\\n{feature}:\")\n",
    "    print(f\"  Lower bound: {lower_bound:.4f}\")\n",
    "    print(f\"  Upper bound: {upper_bound:.4f}\")\n",
    "    print(f\"  Number of outliers: {len(outliers)} ({len(outliers)/len(df)*100:.2f}%)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. While there are values outside 1.5 std. dev. out of the IQR ranges, they don't appear to be data misinuputs or observations outside the realm of possibility. As we look at default probability, keeping these extreme distributions makes sense, as a lot of the risk is held in these wide tails. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Correlations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part II: Training (50 points)\n",
    "\n",
    "We've familiarized ourselves with the data, so now we're going to train some models to model the probability of default. \n",
    "Use *wkta, reta, ebitta* and *mv* as features in the following models:\n",
    "- Model 1: The logistic classifier \n",
    "- Model 2: The decision tree\n",
    "- Model 3: A neural network\n",
    "- Model 4: Gradient Boosting for Classicifation\n",
    "\n",
    "Split the data into a random training, validation and test using using the 60/20/20 rule. Pin down your random sets by providing the seed.\n",
    "If there are hyperparameters, tune these in the correct way and show plots - using an appropriate measure - to explain your final hyperparameter(s). Explain why you have used this measure!\n",
    "\n",
    "Create **Table 2** by showing the accuracy, recall, precision, F1 score, and the $F_\\beta$ using the test set for each model.\n",
    "Choose your own $\\beta$ with explanation. Interpret the outcomes. Did you expect these results? Why/Why NOT?\n",
    "\n",
    "### More information about Model 2\n",
    "Estimate a dicision tree using the default hyperparameteres of Python. Just tune the *maximum depth* parameter. \n",
    "\n",
    "### More information about Model 3\n",
    "Set the following conditions fixed: \n",
    "- **Activation function**: *Relu* for hidden layers\n",
    "- **dropout ratio**: put this after each hidden layer and set it equal to 0.2\n",
    "- **epochs**: 50\n",
    "- **batch size**: 10\n",
    "\n",
    "When compiling the model, set the following conditions fixed:\n",
    "- Use loss = 'categorical_crossentropy'\n",
    "- optimizer='adam'\n",
    "- metrics=accuracy\n",
    "\n",
    "Now tune the following hyperparameters: \n",
    "- the number of **hidden** layers: 1 or 2.\n",
    "- Number of nodes per hidden layer: 16 or 8. \n",
    "\n",
    "### More information about Model 4\n",
    "Set the following hyperparameters fixed:\n",
    "- Learning rate: 0.8\n",
    "- random state: 0\n",
    "- max depth: 2\n",
    "\n",
    "Tune the the parameter *n_estimators*.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model training\n",
    "\n",
    "\n",
    "# Data preparation \n",
    "X = df[['wkta', 'reta', 'ebitta', 'mv']]\n",
    "y = df['def']\n",
    "\n",
    "# Splitting the data into training (60%), validation (20%), and testing (20%) sets randomly\n",
    "X_temp, X_test, y_temp, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_temp, y_temp, test_size=0.25, random_state=42, stratify=y_temp)  # 0.25 x 0.8 = 0.2\n",
    "\n",
    "# Scale the features \n",
    "# tree models don’t need scaling, but NN and logistic do\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_val_scaled   = scaler.transform(X_val)\n",
    "X_test_scaled  = scaler.transform(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model 1: Logistic Regression\n",
    "\n",
    "log_reg = LogisticRegression(max_iter=200)\n",
    "log_reg.fit(X_train_scaled, y_train)\n",
    "\n",
    "y_pred_log = log_reg.predict(X_test_scaled)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model 2: Decision Tree\n",
    "# Hyperparameter tuning for max_depth\n",
    "\n",
    "# Depth values: 1 to 15\n",
    "depths = range(1, 16)\n",
    "val_scores = []\n",
    "\n",
    "# Tune max_depth on validation set\n",
    "for d in depths:\n",
    "    tree = DecisionTreeClassifier(max_depth=d, random_state=0)\n",
    "    tree.fit(X_train, y_train)\n",
    "    preds = tree.predict(X_val)\n",
    "    val_scores.append(f1_score(y_val, preds))\n",
    "\n",
    "# Plot tuning curve\n",
    "plt.figure(figsize=(6,4))\n",
    "plt.plot(depths, val_scores, marker='o')\n",
    "plt.xlabel(\"max_depth\")\n",
    "plt.ylabel(\"Validation F1 Score\")\n",
    "plt.title(\"Decision Tree Hyperparameter Tuning\")\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Select best depth\n",
    "best_depth = depths[np.argmax(val_scores)]\n",
    "dt = DecisionTreeClassifier(max_depth=best_depth, random_state=0)\n",
    "dt.fit(X_train, y_train)\n",
    "y_pred_dt = dt.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model 3: Neural Network\n",
    "# Hyperparameter tuning for number of layers and neurons per layer\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Input\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras import backend as K\n",
    "\n",
    "# --- prepare labels for categorical crossentropy ---\n",
    "y_train_cat = to_categorical(y_train, num_classes=2)\n",
    "y_val_cat   = to_categorical(y_val, num_classes=2)\n",
    "y_test_cat  = to_categorical(y_test, num_classes=2)\n",
    "\n",
    "def build_model(n_layers, n_nodes):\n",
    "    # Clear previous graph just in case (helps prevent weird crashes in notebooks)\n",
    "    K.clear_session()\n",
    "\n",
    "    model = Sequential()\n",
    "    # Explicit Input layer (no warning now)\n",
    "    model.add(Input(shape=(X_train_scaled.shape[1],)))\n",
    "    model.add(Dense(n_nodes, activation='relu'))\n",
    "    model.add(Dropout(0.2))\n",
    "\n",
    "    if n_layers == 2:\n",
    "        model.add(Dense(n_nodes, activation='relu'))\n",
    "        model.add(Dropout(0.2))\n",
    "\n",
    "    model.add(Dense(2, activation='softmax'))\n",
    "\n",
    "    model.compile(\n",
    "        loss='categorical_crossentropy',\n",
    "        optimizer='adam',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    return model\n",
    "\n",
    "# --- hyperparameter search ---\n",
    "configs = [(1, 16), (1, 8), (2, 16), (2, 8)]\n",
    "val_acc_results = {}\n",
    "histories = {}\n",
    "\n",
    "for layers, nodes in configs:\n",
    "    print(f\"Training NN config: layers={layers}, nodes={nodes}\")\n",
    "    model = build_model(layers, nodes)\n",
    "    history = model.fit(\n",
    "        X_train_scaled, y_train_cat,\n",
    "        validation_data=(X_val_scaled, y_val_cat),\n",
    "        epochs=50,\n",
    "        batch_size=10,\n",
    "        verbose=0\n",
    "    )\n",
    "    val_acc = max(history.history['val_accuracy'])\n",
    "    val_acc_results[(layers, nodes)] = val_acc\n",
    "    histories[(layers, nodes)] = history\n",
    "\n",
    "best_config = max(val_acc_results, key=val_acc_results.get)\n",
    "print(\"Best NN config:\", best_config)\n",
    "\n",
    "# --- retrain best model on train+val (optional but nice) ---\n",
    "X_train_full_scaled = np.vstack([X_train_scaled, X_val_scaled])\n",
    "y_train_full_cat    = to_categorical(\n",
    "    np.concatenate([y_train.values, y_val.values]),\n",
    "    num_classes=2\n",
    ")\n",
    "\n",
    "best_nn = build_model(*best_config)\n",
    "best_nn.fit(\n",
    "    X_train_full_scaled, y_train_full_cat,\n",
    "    epochs=50,\n",
    "    batch_size=10,\n",
    "    verbose=0\n",
    ")\n",
    "\n",
    "nn_pred_prob = best_nn.predict(X_test_scaled)\n",
    "y_pred_nn = np.argmax(nn_pred_prob, axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model 4: Gradient Boosting\n",
    "# Hyperparameter tuning for n_estimators\n",
    "\n",
    "estimators = [10, 20, 40, 60, 80, 100, 150]\n",
    "val_scores_gb = []\n",
    "\n",
    "# Tune n_estimators on validation set\n",
    "for n in estimators:\n",
    "    gb = GradientBoostingClassifier(\n",
    "        n_estimators=n,\n",
    "        learning_rate=0.8,\n",
    "        max_depth=2,\n",
    "        random_state=0\n",
    "    )\n",
    "    gb.fit(X_train, y_train)\n",
    "    preds = gb.predict(X_val)\n",
    "    val_scores_gb.append(f1_score(y_val, preds))\n",
    "\n",
    "# Plot tuning curve\n",
    "plt.figure(figsize=(6,4))\n",
    "plt.plot(estimators, val_scores_gb, marker='o')\n",
    "plt.xlabel(\"n_estimators\")\n",
    "plt.ylabel(\"Validation F1 Score\")\n",
    "plt.title(\"Gradient Boosting Tuning\")\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Fit best model with optimal n_estimators and predefined hyperparameters (learning_rate=0.8, max_depth=2)\n",
    "best_n = estimators[np.argmax(val_scores_gb)]\n",
    "gb = GradientBoostingClassifier(\n",
    "    n_estimators=best_n,\n",
    "    learning_rate=0.8,\n",
    "    max_depth=2,\n",
    "    random_state=0\n",
    ")\n",
    "gb.fit(X_train, y_train)\n",
    "\n",
    "y_pred_gb = gb.predict(X_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Metrics Table\n",
    "\n",
    "models = {\n",
    "    \"Logistic\": y_pred_log,\n",
    "    \"Decision Tree\": y_pred_dt,\n",
    "    \"Neural Network\": y_pred_nn,\n",
    "    \"Gradient Boosting\": y_pred_gb\n",
    "}\n",
    "\n",
    "results = []\n",
    "\n",
    "for name, preds in models.items():\n",
    "    acc = accuracy_score(y_test, preds)\n",
    "    prec = precision_score(y_test, preds)\n",
    "    rec = recall_score(y_test, preds)\n",
    "    f1 = f1_score(y_test, preds)\n",
    "    fb = fbeta_score(y_test, preds, beta=2)\n",
    "\n",
    "    results.append([name, acc, prec, rec, f1, fb])\n",
    "\n",
    "df_results = pd.DataFrame(results, columns=[\n",
    "    \"Model\", \"Accuracy\", \"Precision\", \"Recall\", \"F1\", \"Fβ (β=2)\"\n",
    "])\n",
    "\n",
    "print(df_results)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part III: Estimating a tree using only two features (30 points)\n",
    "\n",
    "Select only **wkta** and **reta** as features. Then estimate again a decision tree, again tuning the *maximum depth* hyperparameter.\n",
    "Lets call this model **Decision Tree Small**\n",
    "\n",
    "Answer the following questions:\n",
    "- Is the hyperparameter of your decision tree of Part II robust?\n",
    "- Does Decision Tree Small outperform the best model of Part II?\n",
    "- Does Decision Tree Small beat the logit *with the same two features*?\n",
    "\n",
    "Also make a plot of your final decision tree with the two features as done in the tutorial/lecture. Interpret this figure. \n",
    "\n",
    "Put your test results in **Table 3**. Interpret your results and relate these to main question of the case.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Discussion (5 points)\n",
    "\n",
    "Provide a short discussion about the way we tune the hyperparameters. Pay attention to the charactaristics of the data at hand while answering this question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mbh-xgb",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
